{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1678135788769}],"collapsed_sections":["4JnkELT0cIJg","ZY2VXXXu74w5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1f7df330663048998adcf8a45bc8f69b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e896e6096dd244c59d7955e2035cd729","IPY_MODEL_a6ff238c29984b24bf6d0bd175c19430","IPY_MODEL_3c085ba3f3fd4c3c8a6bb41b41ce1479"],"layout":"IPY_MODEL_16b0c8aa6e0f427e8a54d3791abb7504"}},"e896e6096dd244c59d7955e2035cd729":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7b2dd0f78384cad8e400b282996cdf5","placeholder":"​","style":"IPY_MODEL_6a27e43b0e434edd82ee63f0a91036ca","value":"100%"}},"a6ff238c29984b24bf6d0bd175c19430":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cce0e6c0c4ec442cb47e65c674e02e92","max":818322941,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5b9f38e2f0d4f9aa97fe87265263743","value":818322941}},"3c085ba3f3fd4c3c8a6bb41b41ce1479":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df554fb955c7454696beac5a82889386","placeholder":"​","style":"IPY_MODEL_74e9112a87a242f4831b7d68c7da6333","value":" 780M/780M [00:05&lt;00:00, 126MB/s]"}},"16b0c8aa6e0f427e8a54d3791abb7504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7b2dd0f78384cad8e400b282996cdf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a27e43b0e434edd82ee63f0a91036ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cce0e6c0c4ec442cb47e65c674e02e92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b9f38e2f0d4f9aa97fe87265263743":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df554fb955c7454696beac5a82889386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74e9112a87a242f4831b7d68c7da6333":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"t6MPjfT5NrKQ"},"source":["<div align=\"center\">\n","\n","  <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n","    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov5/v70/splash.png\"></a>\n","\n","\n","<br>\n","  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n","  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n","  <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n","<br>\n","\n","This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> 🚀 notebook by <a href=\"https://ultralytics.com\">Ultralytics</a> presents simple train, validate and predict examples to help start your AI adventure.<br>See <a href=\"https://github.com/ultralytics/yolov5/issues/new/choose\">GitHub</a> for community support or <a href=\"https://ultralytics.com/contact\">contact us</a> for professional support.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["# Setup\n","\n","Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."]},{"cell_type":"code","metadata":{"id":"wbvMlHd_QwMG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"09651938-d1d4-4425-b442-70699c2a1a06","executionInfo":{"status":"ok","timestamp":1681554040066,"user_tz":-60,"elapsed":27012,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}}},"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","display = utils.notebook_init()  # checks"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 🚀 v7.0-145-g94714fe Python-3.9.16 torch-2.0.0+cu118 CPU\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.2/225.8 GB disk)\n"]}]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hf7IL0uwUxiO","executionInfo":{"status":"ok","timestamp":1680897960980,"user_tz":-60,"elapsed":400,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"64f427ae-8828-47ce-cd9a-f4c83f2ec372"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n"]}]},{"cell_type":"code","source":["!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n","\n","!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n","\n","!sudo apt-get update\n","\n","!sudo apt-get install edgetpu-compiler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w8no_zHnLaF","executionInfo":{"status":"ok","timestamp":1680697296413,"user_tz":-60,"elapsed":7148,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"52fd494f-d00e-4490-9727-38e7d075f5a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1210  100  1210    0     0  28809      0 --:--:-- --:--:-- --:--:-- 28809\n","OK\n","deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n","Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n","Hit:2 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease\n","Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n","Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n","Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n","Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n","Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Hit:12 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","edgetpu-compiler is already the newest version (16.0).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-525\n","Use 'sudo apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"]}]},{"cell_type":"code","source":["!sudo apt upgrade"],"metadata":{"id":"JJvvRWM-XaAp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FfZW7rz4NIoD","executionInfo":{"status":"ok","timestamp":1681554258073,"user_tz":-60,"elapsed":192360,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"8c3208aa-95a9-4335-e2f0-e59b86e55dc6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","from utils.torch_utils import prune\n","model = torch.load(\"yolov5n-diorship.pt\")\n","prune(model,0.3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"iXeJwzTrCcgE","executionInfo":{"status":"error","timestamp":1680619745913,"user_tz":-60,"elapsed":316,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"ac729c40-929e-458f-e006-f5530773d988"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-be853a1a6974>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov5n-diorship.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/yolov5/utils/torch_utils.py\u001b[0m in \u001b[0;36mprune\u001b[0;34m(model, amount)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Prune model to requested global sparsity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mprune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_unstructured\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'named_modules'"]}]},{"cell_type":"code","source":["!edgetpu_compiler -sad yolov5n-diorship-int8.tflite"],"metadata":{"id":"b0rcCJ-p2SVg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680698415776,"user_tz":-60,"elapsed":4353,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"ffb5a821-5b81-49c4-ba2a-0d533d1140cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Edge TPU Compiler version 16.0.384591198\n","Searching for valid delegate with step 1\n","Try to compile segment with 261 ops\n","Started a compilation timeout timer of 180 seconds.\n","\n","Model compiled successfully in 4071 ms.\n","\n","Input model: yolov5n-diorship-int8.tflite\n","Input size: 1.89MiB\n","Output model: yolov5n-diorship-int8_edgetpu.tflite\n","Output size: 2.34MiB\n","On-chip memory used for caching model parameters: 1.83MiB\n","On-chip memory remaining for caching model parameters: 5.20MiB\n","Off-chip memory used for streaming uncached model parameters: 41.75KiB\n","Number of Edge TPU subgraphs: 1\n","Total number of operations: 261\n","Operation log: yolov5n-diorship-int8_edgetpu.log\n","\n","Operator                       Count      Status\n","\n","RESIZE_NEAREST_NEIGHBOR        2          Mapped to Edge TPU\n","MUL                            75         Mapped to Edge TPU\n","QUANTIZE                       6          Mapped to Edge TPU\n","STRIDED_SLICE                  9          Mapped to Edge TPU\n","CONV_2D                        60         Mapped to Edge TPU\n","RESHAPE                        6          Mapped to Edge TPU\n","ADD                            10         Mapped to Edge TPU\n","MAX_POOL_2D                    3          Mapped to Edge TPU\n","PAD                            7          Mapped to Edge TPU\n","CONCATENATION                  17         Mapped to Edge TPU\n","LOGISTIC                       66         Mapped to Edge TPU\n","Compilation child process completed within timeout period.\n","Compilation succeeded! \n"]}]},{"cell_type":"code","source":["!python export.py --data DIORship.yaml --weights yolov5n-diorship.pt --include tflite --imgsz 448  #448 largest"],"metadata":{"id":"uI_WqwHgRDHR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681555059683,"user_tz":-60,"elapsed":81857,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"9e422299-92cb-48b7-8959-70c17f0092d1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mexport: \u001b[0mdata=DIORship.yaml, weights=['yolov5n-diorship.pt'], imgsz=[448], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['tflite']\n","YOLOv5 🚀 v7.0-145-g94714fe Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","Fusing layers... \n","Model summary: 157 layers, 1760518 parameters, 0 gradients\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5n-diorship.pt with output shape (1, 12348, 6) (3.8 MB)\n","\n","\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.12.0...\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n","  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n","  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n","  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  4                -1  1     29184  models.common.C3                        [64, 64, 2]                   \n","  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  6                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n","  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n","  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n"," 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n"," 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 24      [17, 20, 23]  1      8118  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256], [448, 448]]\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(1, 448, 448, 3)]   0           []                               \n","                                                                                                  \n"," tf_conv (TFConv)               (1, 224, 224, 16)    1744        ['input_1[0][0]']                \n","                                                                                                  \n"," tf_conv_1 (TFConv)             (1, 112, 112, 32)    4640        ['tf_conv[0][0]']                \n","                                                                                                  \n"," tfc3 (TFC3)                    (1, 112, 112, 32)    4704        ['tf_conv_1[0][0]']              \n","                                                                                                  \n"," tf_conv_7 (TFConv)             (1, 56, 56, 64)      18496       ['tfc3[0][0]']                   \n","                                                                                                  \n"," tfc3_1 (TFC3)                  (1, 56, 56, 64)      28928       ['tf_conv_7[0][0]']              \n","                                                                                                  \n"," tf_conv_15 (TFConv)            (1, 28, 28, 128)     73856       ['tfc3_1[0][0]']                 \n","                                                                                                  \n"," tfc3_2 (TFC3)                  (1, 28, 28, 128)     156288      ['tf_conv_15[0][0]']             \n","                                                                                                  \n"," tf_conv_25 (TFConv)            (1, 14, 14, 256)     295168      ['tfc3_2[0][0]']                 \n","                                                                                                  \n"," tfc3_3 (TFC3)                  (1, 14, 14, 256)     295680      ['tf_conv_25[0][0]']             \n","                                                                                                  \n"," tfsppf (TFSPPF)                (1, 14, 14, 256)     164224      ['tfc3_3[0][0]']                 \n","                                                                                                  \n"," tf_conv_33 (TFConv)            (1, 14, 14, 128)     32896       ['tfsppf[0][0]']                 \n","                                                                                                  \n"," tf_upsample (TFUpsample)       (1, 28, 28, 128)     0           ['tf_conv_33[0][0]']             \n","                                                                                                  \n"," tf_concat (TFConcat)           (1, 28, 28, 256)     0           ['tf_upsample[0][0]',            \n","                                                                  'tfc3_2[0][0]']                 \n","                                                                                                  \n"," tfc3_4 (TFC3)                  (1, 28, 28, 128)     90496       ['tf_concat[0][0]']              \n","                                                                                                  \n"," tf_conv_39 (TFConv)            (1, 28, 28, 64)      8256        ['tfc3_4[0][0]']                 \n","                                                                                                  \n"," tf_upsample_1 (TFUpsample)     (1, 56, 56, 64)      0           ['tf_conv_39[0][0]']             \n","                                                                                                  \n"," tf_concat_1 (TFConcat)         (1, 56, 56, 128)     0           ['tf_upsample_1[0][0]',          \n","                                                                  'tfc3_1[0][0]']                 \n","                                                                                                  \n"," tfc3_5 (TFC3)                  (1, 56, 56, 64)      22720       ['tf_concat_1[0][0]']            \n","                                                                                                  \n"," tf_conv_45 (TFConv)            (1, 28, 28, 64)      36928       ['tfc3_5[0][0]']                 \n","                                                                                                  \n"," tf_concat_2 (TFConcat)         (1, 28, 28, 128)     0           ['tf_conv_45[0][0]',             \n","                                                                  'tf_conv_39[0][0]']             \n","                                                                                                  \n"," tfc3_6 (TFC3)                  (1, 28, 28, 128)     74112       ['tf_concat_2[0][0]']            \n","                                                                                                  \n"," tf_conv_51 (TFConv)            (1, 14, 14, 128)     147584      ['tfc3_6[0][0]']                 \n","                                                                                                  \n"," tf_concat_3 (TFConcat)         (1, 14, 14, 256)     0           ['tf_conv_51[0][0]',             \n","                                                                  'tf_conv_33[0][0]']             \n","                                                                                                  \n"," tfc3_7 (TFC3)                  (1, 14, 14, 256)     295680      ['tf_concat_3[0][0]']            \n","                                                                                                  \n"," tf_detect (TFDetect)           ((1, 12348, 6),      8118        ['tfc3_5[0][0]',                 \n","                                )                                 'tfc3_6[0][0]',                 \n","                                                                  'tfc3_7[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,760,518\n","Trainable params: 0\n","Non-trainable params: 1,760,518\n","__________________________________________________________________________________________________\n","\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 8.8s, saved as yolov5n-diorship_saved_model (6.9 MB)\n","\n","\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.12.0...\n","WARNING:absl:Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 268). These functions will not be directly callable after loading.\n","\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ✅ 63.1s, saved as yolov5n-diorship-fp16.tflite (3.5 MB)\n","\n","Export complete (72.4s)\n","Results saved to \u001b[1m/content/yolov5\u001b[0m\n","Detect:          python detect.py --weights yolov5n-diorship-fp16.tflite \n","Validate:        python val.py --weights yolov5n-diorship-fp16.tflite \n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5n-diorship-fp16.tflite')  \n","Visualize:       https://netron.app\n"]}]},{"cell_type":"code","source":["!python export.py --weights yolov5n-diorall.pt --include edgetpu --imgsz 256 --data DIOR.yaml #256 best"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuG9kDQeoVJ-","executionInfo":{"status":"ok","timestamp":1680699021980,"user_tz":-60,"elapsed":131960,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"711d4d65-7565-4195-95c9-ca138eda322f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mexport: \u001b[0mdata=DIOR.yaml, weights=['yolov5n-diorall.pt'], imgsz=[240], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['edgetpu']\n","YOLOv5 🚀 v7.0-134-g23c4923 Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","Fusing layers... \n","Model summary: 157 layers, 1786225 parameters, 0 gradients\n","WARNING ⚠️ --img-size 240 must be multiple of max stride 32, updating to 256\n","WARNING ⚠️ --img-size 240 must be multiple of max stride 32, updating to 256\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5n-diorall.pt with output shape (1, 4032, 25) (3.9 MB)\n","2023-04-05 12:48:21.534599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-05 12:48:23.327134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.12.0...\n","\n","                 from  n    params  module                                  arguments                     \n","2023-04-05 12:48:25.287434: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n","  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n","  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n","  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  4                -1  1     29184  models.common.C3                        [64, 64, 2]                   \n","  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  6                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n","  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n","  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n"," 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n"," 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 24      [17, 20, 23]  1     33825  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256], [256, 256]]\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(1, 256, 256, 3)]   0           []                               \n","                                                                                                  \n"," tf_conv (TFConv)               (1, 128, 128, 16)    1744        ['input_1[0][0]']                \n","                                                                                                  \n"," tf_conv_1 (TFConv)             (1, 64, 64, 32)      4640        ['tf_conv[0][0]']                \n","                                                                                                  \n"," tfc3 (TFC3)                    (1, 64, 64, 32)      4704        ['tf_conv_1[0][0]']              \n","                                                                                                  \n"," tf_conv_7 (TFConv)             (1, 32, 32, 64)      18496       ['tfc3[0][0]']                   \n","                                                                                                  \n"," tfc3_1 (TFC3)                  (1, 32, 32, 64)      28928       ['tf_conv_7[0][0]']              \n","                                                                                                  \n"," tf_conv_15 (TFConv)            (1, 16, 16, 128)     73856       ['tfc3_1[0][0]']                 \n","                                                                                                  \n"," tfc3_2 (TFC3)                  (1, 16, 16, 128)     156288      ['tf_conv_15[0][0]']             \n","                                                                                                  \n"," tf_conv_25 (TFConv)            (1, 8, 8, 256)       295168      ['tfc3_2[0][0]']                 \n","                                                                                                  \n"," tfc3_3 (TFC3)                  (1, 8, 8, 256)       295680      ['tf_conv_25[0][0]']             \n","                                                                                                  \n"," tfsppf (TFSPPF)                (1, 8, 8, 256)       164224      ['tfc3_3[0][0]']                 \n","                                                                                                  \n"," tf_conv_33 (TFConv)            (1, 8, 8, 128)       32896       ['tfsppf[0][0]']                 \n","                                                                                                  \n"," tf_upsample (TFUpsample)       (1, 16, 16, 128)     0           ['tf_conv_33[0][0]']             \n","                                                                                                  \n"," tf_concat (TFConcat)           (1, 16, 16, 256)     0           ['tf_upsample[0][0]',            \n","                                                                  'tfc3_2[0][0]']                 \n","                                                                                                  \n"," tfc3_4 (TFC3)                  (1, 16, 16, 128)     90496       ['tf_concat[0][0]']              \n","                                                                                                  \n"," tf_conv_39 (TFConv)            (1, 16, 16, 64)      8256        ['tfc3_4[0][0]']                 \n","                                                                                                  \n"," tf_upsample_1 (TFUpsample)     (1, 32, 32, 64)      0           ['tf_conv_39[0][0]']             \n","                                                                                                  \n"," tf_concat_1 (TFConcat)         (1, 32, 32, 128)     0           ['tf_upsample_1[0][0]',          \n","                                                                  'tfc3_1[0][0]']                 \n","                                                                                                  \n"," tfc3_5 (TFC3)                  (1, 32, 32, 64)      22720       ['tf_concat_1[0][0]']            \n","                                                                                                  \n"," tf_conv_45 (TFConv)            (1, 16, 16, 64)      36928       ['tfc3_5[0][0]']                 \n","                                                                                                  \n"," tf_concat_2 (TFConcat)         (1, 16, 16, 128)     0           ['tf_conv_45[0][0]',             \n","                                                                  'tf_conv_39[0][0]']             \n","                                                                                                  \n"," tfc3_6 (TFC3)                  (1, 16, 16, 128)     74112       ['tf_concat_2[0][0]']            \n","                                                                                                  \n"," tf_conv_51 (TFConv)            (1, 8, 8, 128)       147584      ['tfc3_6[0][0]']                 \n","                                                                                                  \n"," tf_concat_3 (TFConcat)         (1, 8, 8, 256)       0           ['tf_conv_51[0][0]',             \n","                                                                  'tf_conv_33[0][0]']             \n","                                                                                                  \n"," tfc3_7 (TFC3)                  (1, 8, 8, 256)       295680      ['tf_concat_3[0][0]']            \n","                                                                                                  \n"," tf_detect (TFDetect)           ((1, 4032, 25),      33825       ['tfc3_5[0][0]',                 \n","                                )                                 'tfc3_6[0][0]',                 \n","                                                                  'tfc3_7[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1,786,225\n","Trainable params: 0\n","Non-trainable params: 1,786,225\n","__________________________________________________________________________________________________\n","2023-04-05 12:48:29.526488: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n","2023-04-05 12:48:29.526906: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n","\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 11.7s, saved as yolov5n-diorall_saved_model (7.0 MB)\n","\n","\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.12.0...\n","WARNING:absl:Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 268). These functions will not be directly callable after loading.\n","2023-04-05 12:49:29.056547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:29.129973: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:29.543565: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:30.786369: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:30.911912: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:30.947629: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:30.957355: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:31.002886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:31.009952: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:31.231466: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:31.441187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:31.705225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:31.783084: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:31.796766: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:31.841979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:32.078824: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:32.123169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:32.215868: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:32.241865: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:32.269694: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:32.507641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:32.762452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:32.997755: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:33.001986: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:33.214815: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:33.331314: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:33.417492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:33.571205: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:33.674839: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:33.932970: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:34.140950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:34.304416: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:34.341829: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:34.457038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:34.594770: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:35.135161: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:35.151641: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:35.235881: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:35.267082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:35.382367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:35.482977: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:35.515246: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:35.696841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:35.711153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:35.735626: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:35.788565: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:35.991353: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:35.997276: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:36.196651: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:36.209163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:36.444432: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node pad_paddings}}]]\n","2023-04-05 12:49:36.625140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:36.642173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:36.796458: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:37.222921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:37.223312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_1_sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_1_sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:37.224188: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_7_sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_7_sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:37.225293: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_15_sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_15_sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:37.226590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_25_sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_25_sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:37.229221: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_45_sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_45_sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:37.230109: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_51_sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_51_sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:38.524975: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:38.591705: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:38.592096: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_1_sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_1_sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:38.593006: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_7_sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_7_sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:38.594121: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_15_sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_15_sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:38.595482: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_25_sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_25_sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:38.598264: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_45_sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_45_sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:38.599189: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'tf_conv_51_sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node tf_conv_51_sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:38.766153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:38.856904: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_sequential_tf_pad_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_sequential_tf_pad_pad_paddings}}]]\n","2023-04-05 12:49:38.857285: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_1_sequential_1_tf_pad_1_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_1_sequential_1_tf_pad_1_pad_paddings}}]]\n","2023-04-05 12:49:38.858204: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_7_sequential_3_tf_pad_2_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_7_sequential_3_tf_pad_2_pad_paddings}}]]\n","2023-04-05 12:49:38.859322: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_15_sequential_5_tf_pad_3_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_15_sequential_5_tf_pad_3_pad_paddings}}]]\n","2023-04-05 12:49:38.860666: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_25_sequential_7_tf_pad_4_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_25_sequential_7_tf_pad_4_pad_paddings}}]]\n","2023-04-05 12:49:38.863365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_45_sequential_11_tf_pad_5_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_45_sequential_11_tf_pad_5_pad_paddings}}]]\n","2023-04-05 12:49:38.864318: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'model_tf_conv_51_sequential_13_tf_pad_6_pad_paddings' with dtype int32 and shape [4,2]\n","\t [[{{node model_tf_conv_51_sequential_13_tf_pad_6_pad_paddings}}]]\n","2023-04-05 12:49:42.359689: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n","2023-04-05 12:49:42.359780: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n","2023-04-05 12:49:42.361172: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp3a4gzvmv\n","2023-04-05 12:49:42.409084: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n","2023-04-05 12:49:42.409183: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp3a4gzvmv\n","2023-04-05 12:49:42.775609: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n","2023-04-05 12:49:42.814380: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n","2023-04-05 12:49:43.286476: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp3a4gzvmv\n","2023-04-05 12:49:43.509865: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 1148699 microseconds.\n","2023-04-05 12:49:44.434636: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-04-05 12:49:45.423339: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2116] Estimated count of arithmetic ops: 805.918 M  ops, equivalently 402.959 M  MACs\n","fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n","2023-04-05 12:50:11.249723: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2116] Estimated count of arithmetic ops: 805.918 M  ops, equivalently 402.959 M  MACs\n","\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ✅ 99.4s, saved as yolov5n-diorall-int8.tflite (1.9 MB)\n","\n","\u001b[34m\u001b[1mEdge TPU:\u001b[0m starting export with Edge TPU compiler 16.0.384591198...\n","Edge TPU Compiler version 16.0.384591198\n","Searching for valid delegate with step 10\n","Try to compile segment with 262 ops\n","Started a compilation timeout timer of 180 seconds.\n","\n","Model compiled successfully in 2524 ms.\n","\n","Input model: yolov5n-diorall-int8.tflite\n","Input size: 1.91MiB\n","Output model: ./yolov5n-diorall-int8_edgetpu.tflite\n","Output size: 2.08MiB\n","On-chip memory used for caching model parameters: 1.86MiB\n","On-chip memory remaining for caching model parameters: 5.79MiB\n","Off-chip memory used for streaming uncached model parameters: 15.75KiB\n","Number of Edge TPU subgraphs: 1\n","Total number of operations: 262\n","Operation log: ./yolov5n-diorall-int8_edgetpu.log\n","\n","Operator                       Count      Status\n","\n","STRIDED_SLICE                  9          Mapped to Edge TPU\n","RESHAPE                        6          Mapped to Edge TPU\n","CONCATENATION                  17         Mapped to Edge TPU\n","PAD                            7          Mapped to Edge TPU\n","LOGISTIC                       66         Mapped to Edge TPU\n","MAX_POOL_2D                    3          Mapped to Edge TPU\n","ADD                            10         Mapped to Edge TPU\n","CONV_2D                        60         Mapped to Edge TPU\n","QUANTIZE                       7          Mapped to Edge TPU\n","MUL                            75         Mapped to Edge TPU\n","RESIZE_NEAREST_NEIGHBOR        2          Mapped to Edge TPU\n","Compilation child process completed within timeout period.\n","Compilation succeeded! \n","\u001b[34m\u001b[1mEdge TPU:\u001b[0m export success ✅ 3.0s, saved as yolov5n-diorall-int8_edgetpu.tflite (2.1 MB)\n","\n","Export complete (114.7s)\n","Results saved to \u001b[1m/content/yolov5\u001b[0m\n","Detect:          python detect.py --weights yolov5n-diorall-int8_edgetpu.tflite \n","Validate:        python val.py --weights yolov5n-diorall-int8_edgetpu.tflite \n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5n-diorall-int8_edgetpu.tflite')  \n","Visualize:       https://netron.app\n"]}]},{"cell_type":"code","source":["!python val.py --task test --weights  yolov5n-diorship-int8-448.tflite --data DIOR.yaml --imgsz 448"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbOR8CiH3CMd","executionInfo":{"status":"ok","timestamp":1680883421293,"user_tz":-60,"elapsed":1981981,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"5de87de2-6730-478c-a572-449e97a2dafc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=DIOR.yaml, weights=['yolov5n-diorship-int8-448.tflite'], batch_size=32, imgsz=448, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 🚀 v7.0-134-g23c4923 Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","2023-04-07 15:30:42.730628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-07 15:30:43.742033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Loading yolov5n-diorship-int8-448.tflite for TensorFlow Lite inference...\n","INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","Forcing --batch-size 1 square inference (1,3,448,448) for non-PyTorch models\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/dataset/DIOR/test/labels... 11738 images, 10339 backgrounds, 0 corrupt: 100% 11743/11743 [18:21<00:00, 10.66it/s]\n","\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/drive/MyDrive/Colab Notebooks/dataset/DIOR/test/labels.cache\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11743/11743 [13:18<00:00, 14.70it/s]\n","                   all      11743      35186      0.515      0.517      0.481      0.173\n","             golffield      11743      35186      0.515      0.517      0.481      0.173\n","Speed: 0.3ms pre-process, 41.3ms inference, 0.3ms NMS per image at shape (1, 3, 448, 448)\n","Results saved to \u001b[1mruns/val/exp\u001b[0m\n"]}]},{"cell_type":"code","source":["!python val.py --task test --weights  yolov5n-diorall-fp16.tflite --data DIOR.yaml --imgsz 256"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GXhh-9RT7fT","executionInfo":{"status":"ok","timestamp":1681559170200,"user_tz":-60,"elapsed":1735346,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"f85d6402-4135-41eb-efb0-7ea6964aabc4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=DIOR.yaml, weights=['yolov5n-diorall-fp16.tflite'], batch_size=32, imgsz=256, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-145-g94714fe Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","Loading yolov5n-diorall-fp16.tflite for TensorFlow Lite inference...\n","INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","Forcing --batch-size 1 square inference (1,3,256,256) for non-PyTorch models\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/dataset/DIOR/test/labels... 11738 images, 5 backgrounds, 0 corrupt: 100% 11743/11743 [53:49<00:00,  3.64it/s]\n","\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/drive/MyDrive/Colab Notebooks/dataset/DIOR/test/labels.cache\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11743/11743 [13:12<00:00, 14.82it/s]\n","                   all      11743     124445      0.641      0.317       0.35      0.188\n","             golffield      11743        575      0.373        0.4      0.315      0.137\n","Expressway-toll-station      11743        688      0.948      0.346      0.383      0.274\n","               vehicle      11743      26640       0.79      0.165      0.206      0.104\n","          trainstation      11743        509      0.341      0.295      0.232     0.0801\n","               chimney      11743       1031       0.93      0.269      0.429      0.275\n","           storagetank      11743      23361      0.777      0.288      0.337      0.187\n","                  ship      11743      35186      0.622      0.336      0.375      0.164\n","                harbor      11743       3105      0.525      0.481      0.472      0.244\n","              airplane      11743       8212      0.766      0.262      0.374      0.183\n","      groundtrackfield      11743       1885      0.629      0.411      0.428      0.267\n","           tenniscourt      11743       7343      0.814        0.5      0.572      0.373\n","                   dam      11743        538      0.464      0.348      0.356      0.145\n","       basketballcourt      11743       2146      0.688      0.477      0.519       0.33\n","Expressway-Service-area      11743       1085      0.515      0.232      0.249     0.0794\n","               stadium      11743        672      0.471     0.0789      0.174     0.0847\n","               airport      11743        666      0.173      0.227      0.123     0.0429\n","         baseballfield      11743       3434       0.93      0.383      0.533      0.378\n","                bridge      11743       2589      0.623       0.16      0.173     0.0799\n","              windmill      11743       2998       0.72      0.294      0.333      0.122\n","              overpass      11743       1782      0.713      0.388      0.413      0.214\n","Speed: 0.3ms pre-process, 28.2ms inference, 1.6ms NMS per image at shape (1, 3, 256, 256)\n","Results saved to \u001b[1mruns/val/exp\u001b[0m\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","model_dir = 'yolov5n-diorall_saved_model'\n","tflite_model_file = 'yolov5n-diorall_saved_model.tflite'\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.experimental_new_converter = True\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","\n","tflite_model = converter.convert()\n","\n","with open(tflite_model_file, 'wb') as f:\n","    f.write(tflite_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZ8mW72FoeON","executionInfo":{"status":"ok","timestamp":1680551231961,"user_tz":-60,"elapsed":2946,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"541be01f-a019-483e-d1bf-5b8b0cd379f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Importing a function (__inference_pruned_7112) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"]}]},{"cell_type":"code","source":["!edgetpu_compiler -sad yolov5s.tflite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p42locbpqbBU","executionInfo":{"status":"ok","timestamp":1680612555097,"user_tz":-60,"elapsed":311,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"f985d82b-8a79-4789-d4af-b23feaaa8385"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: edgetpu_compiler: command not found\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"G9A5La7QUmfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!edgetpu_compiler -s -a diorallyolov8n_full_integer_quant-working.tflite"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BRWysp3rBH-","executionInfo":{"status":"ok","timestamp":1680560514049,"user_tz":-60,"elapsed":180420,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"05dcab8a-4393-42ff-d177-3bd262edddbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Edge TPU Compiler version 16.0.384591198\n","Started a compilation timeout timer of 180 seconds.\n","Compilation timeout. This is usually caused by large activations.\n","Compilation failed! \n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","\n","def representative_dataset_gen():\n","    image_directory = \"../drive/MyDrive/Colab Notebooks/dataset/DIOR/valid/image\"  # Replace with your own image directory path\n","    image_paths = [os.path.join(image_directory, file) for file in os.listdir(image_directory) if file.endswith(('.jpg', '.png', '.jpeg'))]\n","\n","    for image_path in image_paths:\n","        # Load and preprocess the image\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = cv2.resize(image, (224, 224))  # Resize to match the input_shape of your model\n","        image = image.astype(np.float32) / 255.0  # Normalize if necessary\n","        image = np.expand_dims(image, axis=0)  # Add batch dimension\n","        yield [image]\n","\n","# Load your model\n","model_path = 'diorshipyolov8n_float32.tflite'\n","tflite_model = None\n","with open(model_path, 'rb') as f:\n","    tflite_model = f.read()\n","\n","# Define the input shape and the number of calibration steps\n","input_shape = (1, 800, 800, 3) # Replace this with the input shape of your model\n","num_calibration_steps = 100\n","\n","# Set up the converter\n","converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = representative_dataset_gen\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","\n","# Convert the model\n","quantized_tflite_model = converter.convert()\n","\n","# Save the quantized model\n","with open('quantized_model_int8.tflite', 'wb') as f:\n","    f.write(quantized_tflite_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"id":"Qhq9OXuauUrN","executionInfo":{"status":"error","timestamp":1680552852131,"user_tz":-60,"elapsed":230,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"f3da5761-bd74-4336-8aa3-8b0d621a508d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-2d436ab405a2>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Set up the converter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentative_dataset_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_saved_model\u001b[0;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m       \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignature_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m       \u001b[0msignature_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    834\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 941\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   debug_info_path = file_io.join(\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot parse file {path_to_pbtxt}: {str(e)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     raise IOError(\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;34mf\"SavedModel file does not exist at: {export_dir}{os.path.sep}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34mf\"{{{constants.SAVED_MODEL_FILENAME_PBTXT}|\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: diorshipyolov8n_float32.tflite/{saved_model.pbtxt|saved_model.pb}"]}]},{"cell_type":"code","source":["!python export.py --weights runs/train/exp6/weights/best.pt --include edgetpu --imgsz 224"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAXOC0xOWda3","executionInfo":{"status":"ok","timestamp":1680647640534,"user_tz":-60,"elapsed":5671,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"8e706884-9d6c-427c-b75b-29a841bfb685"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['runs/train/exp6/weights/best.pt'], imgsz=[224], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['edgetpu']\n","YOLOv5 🚀 v7.0-134-g23c4923 Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","Traceback (most recent call last):\n","  File \"/content/yolov5/export.py\", line 672, in <module>\n","    main(opt)\n","  File \"/content/yolov5/export.py\", line 667, in main\n","    run(**vars(opt))\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/yolov5/export.py\", line 547, in run\n","    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n","  File \"/content/yolov5/models/experimental.py\", line 79, in attempt_load\n","    ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 791, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 252, in __init__\n","    super().__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/exp6/weights/best.pt'\n"]}]},{"cell_type":"code","source":["!python export.py --data DIOR.yaml --weights yolov5n-diorship.pt --include tflite --int8 True --imgsz 400"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u57PKloM8zmF","executionInfo":{"status":"ok","timestamp":1680688136491,"user_tz":-60,"elapsed":8,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}},"outputId":"587a1a9d-f5c0-4538-b2e6-1e1d0cbcfa4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["python3: can't open file '/content/export.py': [Errno 2] No such file or directory\n"]}]},{"cell_type":"code","source":["!python val.py --task test --weights  yolov5n-diorall-int8.tflite --data DIOR.yaml --img 800"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emBM8Z6baOxk","outputId":"30b52805-8fc6-41cc-9c6a-3862f073e555","executionInfo":{"status":"ok","timestamp":1680363377900,"user_tz":-60,"elapsed":3489282,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=DIOR.yaml, weights=['yolov5n-diorall-int8.tflite'], batch_size=32, imgsz=800, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 🚀 v7.0-132-ga82132c Python-3.9.16 torch-1.13.1+cu116 CPU\n","\n","2023-04-01 14:38:11.299674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-01 14:38:12.949031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Loading yolov5n-diorall-int8.tflite for TensorFlow Lite inference...\n","INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","Forcing --batch-size 1 square inference (1,3,800,800) for non-PyTorch models\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning /content/drive/MyDrive/Colab Notebooks/dataset/DIOR/test/labels.cache... 11738 images, 5 backgrounds, 0 corrupt: 100% 11743/11743 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 11743/11743 [57:37<00:00,  3.40it/s]\n","                   all      11743     124445      0.683      0.596      0.612      0.352\n","             golffield      11743        575      0.623      0.647      0.617      0.274\n","Expressway-toll-station      11743        688      0.797      0.515      0.566      0.397\n","               vehicle      11743      26640      0.452      0.426      0.403      0.186\n","          trainstation      11743        509      0.354      0.495      0.339      0.139\n","               chimney      11743       1031      0.915      0.727      0.766      0.562\n","           storagetank      11743      23361      0.553      0.686      0.663      0.342\n","                  ship      11743      35186      0.572      0.813      0.747      0.352\n","                harbor      11743       3105      0.662      0.474       0.51      0.257\n","              airplane      11743       8212       0.77      0.741      0.791      0.446\n","      groundtrackfield      11743       1885      0.617      0.719      0.694      0.421\n","           tenniscourt      11743       7343       0.89      0.843      0.881      0.641\n","                   dam      11743        538      0.398      0.426       0.33      0.144\n","       basketballcourt      11743       2146      0.859      0.861      0.889      0.655\n","Expressway-Service-area      11743       1085      0.879      0.444      0.533      0.308\n","               stadium      11743        672       0.83      0.378      0.601      0.332\n","               airport      11743        666      0.721      0.467      0.542      0.253\n","         baseballfield      11743       3434      0.916      0.673       0.76       0.56\n","                bridge      11743       2589      0.541      0.357       0.35      0.165\n","              windmill      11743       2998      0.675      0.729      0.738      0.326\n","              overpass      11743       1782      0.627      0.507      0.515      0.277\n","Speed: 3.9ms pre-process, 251.5ms inference, 3.3ms NMS per image at shape (1, 3, 800, 800)\n","Results saved to \u001b[1mruns/val/exp8\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"4JnkELT0cIJg"},"source":["# 1. Detect\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","```shell\n","python detect.py --source 0  # webcam\n","                          img.jpg  # image \n","                          vid.mp4  # video\n","                          screen  # screenshot\n","                          path/  # directory\n","                         'path/*.jpg'  # glob\n","                         'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","```"]},{"cell_type":"code","metadata":{"id":"zR9ZbuQCH7FX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27a8c6f9-a7f8-47e5-9750-628cc90c9f4f","executionInfo":{"status":"ok","timestamp":1678664619440,"user_tz":0,"elapsed":5973,"user":{"displayName":"jack spreckley","userId":"14884027367448594892"}}},"source":["!python detect.py --task test --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n","# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: detect.py\n","       [-h]\n","       [--weights WEIGHTS [WEIGHTS ...]]\n","       [--source SOURCE]\n","       [--data DATA]\n","       [--imgsz IMGSZ [IMGSZ ...]]\n","       [--conf-thres CONF_THRES]\n","       [--iou-thres IOU_THRES]\n","       [--max-det MAX_DET]\n","       [--device DEVICE]\n","       [--view-img]\n","       [--save-txt]\n","       [--save-conf]\n","       [--save-crop]\n","       [--nosave]\n","       [--classes CLASSES [CLASSES ...]]\n","       [--agnostic-nms]\n","       [--augment]\n","       [--visualize]\n","       [--update]\n","       [--project PROJECT]\n","       [--name NAME]\n","       [--exist-ok]\n","       [--line-thickness LINE_THICKNESS]\n","       [--hide-labels]\n","       [--hide-conf]\n","       [--half]\n","       [--dnn]\n","       [--vid-stride VID_STRIDE]\n","detect.py: error: unrecognized arguments: --task test\n"]}]},{"cell_type":"markdown","metadata":{"id":"hkAzDWJ7cWTr"},"source":["&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n","<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">"]},{"cell_type":"markdown","metadata":{"id":"0eq1SMWl6Sfn"},"source":["# 2. Validate\n","Validate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag."]},{"cell_type":"code","metadata":{"id":"WQPtK1QYVaD_","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1f7df330663048998adcf8a45bc8f69b","e896e6096dd244c59d7955e2035cd729","a6ff238c29984b24bf6d0bd175c19430","3c085ba3f3fd4c3c8a6bb41b41ce1479","16b0c8aa6e0f427e8a54d3791abb7504","c7b2dd0f78384cad8e400b282996cdf5","6a27e43b0e434edd82ee63f0a91036ca","cce0e6c0c4ec442cb47e65c674e02e92","c5b9f38e2f0d4f9aa97fe87265263743","df554fb955c7454696beac5a82889386","74e9112a87a242f4831b7d68c7da6333"]},"outputId":"c7d0a0d2-abfb-44c3-d60d-f99d0e7aabad"},"source":["# Download COCO val\n","torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n","!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/780M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f7df330663048998adcf8a45bc8f69b"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"ZY2VXXXu74w5"},"source":["# 3. Train\n","\n","<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png\"/></a></p>\n","Close the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n","<br><br>\n","\n","Train a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n","\n","- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n","automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n","- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n","- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n","<br><br>\n","\n","A **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.\n","\n","## Train on Custom Data with Roboflow 🌟 NEW\n","\n","[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package.\n","\n","- Custom Training Example: [https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)\n","- Custom Training Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\n","<br>\n","\n","<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"480\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\"/></a></p>Label images lightning fast (including with model-assisted labeling)"]},{"cell_type":"code","source":["#@title Select YOLOv5 🚀 logger {run: 'auto'}\n","logger = 'ClearML' #@param ['ClearML', 'Comet', 'TensorBoard']\n","\n","if logger == 'ClearML':\n","  %pip install -q clearml\n","  import clearml; clearml.browser_login()\n","elif logger == 'Comet':\n","  %pip install -q comet_ml\n","  import comet_ml; comet_ml.init()\n","elif logger == 'TensorBoard':\n","  %load_ext tensorboard\n","  %tensorboard --logdir runs/train"],"metadata":{"id":"i3oKtE4g-aNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NcFxRcFdJ_O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"721b9028-767f-4a05-c964-692c245f7398"},"source":["# Train YOLOv5s on COCO128 for 3 epochs\n","!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-1-gb32f67f Python-3.7.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\n","Dataset not found ⚠️, missing paths ['/content/datasets/coco128/images/train2017']\n","Downloading https://ultralytics.com/assets/coco128.zip to coco128.zip...\n","100% 6.66M/6.66M [00:00<00:00, 261MB/s]\n","Dataset download success ✅ (0.3s), saved to \u001b[1m/content/datasets\u001b[0m\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n","\n","Transferred 349/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<00:00, 1911.57it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco128/labels/train2017.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:00<00:00, 229.69it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100% 128/128 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:01<00:00, 97.70it/s] \n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to runs/train/exp/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 3 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/2      3.74G    0.04618    0.07207      0.017        232        640: 100% 8/8 [00:07<00:00,  1.10it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  2.28it/s]\n","                   all        128        929      0.672      0.594      0.682      0.451\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/2      5.36G    0.04623    0.06888    0.01821        201        640: 100% 8/8 [00:02<00:00,  3.29it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.17it/s]\n","                   all        128        929      0.721      0.639      0.724       0.48\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/2      5.36G    0.04361    0.06479    0.01698        227        640: 100% 8/8 [00:02<00:00,  3.46it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:01<00:00,  3.11it/s]\n","                   all        128        929      0.758      0.641      0.731      0.487\n","\n","3 epochs completed in 0.005 hours.\n","Optimizer stripped from runs/train/exp/weights/last.pt, 14.9MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 14.9MB\n","\n","Validating runs/train/exp/weights/best.pt...\n","Fusing layers... \n","Model summary: 157 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 4/4 [00:03<00:00,  1.05it/s]\n","                   all        128        929      0.757      0.641      0.732      0.487\n","                person        128        254       0.86      0.705      0.804      0.528\n","               bicycle        128          6      0.773      0.578      0.725      0.426\n","                   car        128         46      0.658      0.435      0.554      0.239\n","            motorcycle        128          5       0.59        0.8      0.837      0.635\n","              airplane        128          6          1      0.996      0.995      0.696\n","                   bus        128          7      0.635      0.714      0.756      0.666\n","                 train        128          3      0.691      0.333      0.753      0.511\n","                 truck        128         12      0.604      0.333      0.472       0.26\n","                  boat        128          6      0.941      0.333       0.46      0.183\n","         traffic light        128         14      0.557      0.183      0.302      0.214\n","             stop sign        128          2      0.827          1      0.995      0.846\n","                 bench        128          9       0.79      0.556      0.677      0.318\n","                  bird        128         16      0.962          1      0.995      0.663\n","                   cat        128          4      0.867          1      0.995      0.754\n","                   dog        128          9          1      0.649      0.903      0.654\n","                 horse        128          2      0.853          1      0.995      0.622\n","              elephant        128         17      0.908      0.882      0.934      0.698\n","                  bear        128          1      0.697          1      0.995      0.995\n","                 zebra        128          4      0.867          1      0.995      0.905\n","               giraffe        128          9      0.788      0.829      0.912      0.701\n","              backpack        128          6      0.841        0.5      0.738      0.311\n","              umbrella        128         18      0.786      0.815      0.859       0.48\n","               handbag        128         19      0.772      0.263      0.366      0.216\n","                   tie        128          7      0.975      0.714       0.77      0.491\n","              suitcase        128          4      0.643       0.75      0.912      0.563\n","               frisbee        128          5       0.72        0.8       0.76      0.717\n","                  skis        128          1      0.748          1      0.995        0.3\n","             snowboard        128          7      0.827      0.686      0.833       0.57\n","           sports ball        128          6      0.637      0.667      0.602      0.311\n","                  kite        128         10      0.645        0.6      0.594      0.224\n","          baseball bat        128          4      0.519      0.278      0.468      0.205\n","        baseball glove        128          7      0.483      0.429      0.465      0.278\n","            skateboard        128          5      0.923        0.6      0.687      0.493\n","         tennis racket        128          7      0.774      0.429      0.544      0.333\n","                bottle        128         18      0.577      0.379      0.551      0.275\n","            wine glass        128         16      0.715      0.875      0.893      0.511\n","                   cup        128         36      0.843      0.667      0.833      0.531\n","                  fork        128          6      0.998      0.333       0.45      0.315\n","                 knife        128         16       0.77      0.688      0.695      0.399\n","                 spoon        128         22      0.839      0.473      0.638      0.383\n","                  bowl        128         28      0.765      0.583      0.715      0.512\n","                banana        128          1      0.903          1      0.995      0.301\n","              sandwich        128          2          1          0      0.359      0.301\n","                orange        128          4      0.718       0.75      0.912      0.581\n","              broccoli        128         11      0.545      0.364       0.43      0.319\n","                carrot        128         24       0.62      0.625      0.724      0.495\n","               hot dog        128          2      0.385          1      0.828      0.762\n","                 pizza        128          5      0.833          1      0.962      0.725\n","                 donut        128         14      0.631          1       0.96      0.833\n","                  cake        128          4      0.871          1      0.995       0.83\n","                 chair        128         35      0.583        0.6      0.608      0.318\n","                 couch        128          6      0.909      0.667      0.813      0.543\n","          potted plant        128         14      0.745      0.786      0.822       0.48\n","                   bed        128          3      0.973      0.333      0.753       0.41\n","          dining table        128         13      0.821      0.356      0.577      0.342\n","                toilet        128          2          1      0.949      0.995      0.797\n","                    tv        128          2      0.566          1      0.995      0.796\n","                laptop        128          3          1          0       0.59      0.311\n","                 mouse        128          2          1          0      0.105     0.0527\n","                remote        128          8          1      0.623      0.634      0.538\n","            cell phone        128          8      0.565      0.375      0.399      0.179\n","             microwave        128          3      0.709          1      0.995      0.736\n","                  oven        128          5      0.328        0.4       0.43      0.282\n","                  sink        128          6      0.438      0.333      0.339      0.266\n","          refrigerator        128          5      0.564        0.8      0.798      0.535\n","                  book        128         29      0.597      0.256      0.351      0.155\n","                 clock        128          9      0.763      0.889      0.934      0.737\n","                  vase        128          2      0.331          1      0.995      0.895\n","              scissors        128          1          1          0      0.497     0.0552\n","            teddy bear        128         21      0.857       0.57      0.837      0.544\n","            toothbrush        128          5      0.799          1      0.928      0.556\n","Results saved to \u001b[1mruns/train/exp\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"15glLzbQx5u0"},"source":["# 4. Visualize"]},{"cell_type":"markdown","source":["## Comet Logging and Visualization 🌟 NEW\n","\n","[Comet](https://www.comet.com/site/lp/yolov5-with-comet/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://www.comet.com/docs/v2/guides/comet-dashboard/code-panels/about-panels/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!\n","\n","Getting started is easy:\n","```shell\n","pip install comet_ml  # 1. install\n","export COMET_API_KEY=<Your API Key>  # 2. paste API key\n","python train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n","```\n","To learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet). If you'd like to learn more about Comet, head over to our [documentation](https://www.comet.com/docs/v2/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=yolov5_colab). Get started by trying out the Comet Colab Notebook:\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\n","\n","<a href=\"https://bit.ly/yolov5-readme-comet2\">\n","<img alt=\"Comet Dashboard\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\" width=\"1280\"/></a>"],"metadata":{"id":"nWOsI5wJR1o3"}},{"cell_type":"markdown","source":["## ClearML Logging and Automation 🌟 NEW\n","\n","[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):\n","\n","- `pip install clearml`\n","- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))\n","\n","You'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\n","\n","You can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml) for details!\n","\n","<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\n","<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>"],"metadata":{"id":"Lay2WsTjNJzP"}},{"cell_type":"markdown","metadata":{"id":"-WPvRbS5Swl6"},"source":["## Local Logging\n","\n","Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n","\n","This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \n","\n","<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"Zelyeqbyt3GD"},"source":["# Environments\n","\n","YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n","\n","- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n","- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n","- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n","- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"6Qu7Iesl0p54"},"source":["# Status\n","\n","![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)\n","\n","If this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n"]},{"cell_type":"markdown","metadata":{"id":"IEijrePND_2I"},"source":["# Appendix\n","\n","Additional content below."]},{"cell_type":"code","metadata":{"id":"GMusP4OAxFu6"},"source":["# YOLOv5 PyTorch HUB Inference (DetectionModels only)\n","import torch\n","\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)  # yolov5n - yolov5x6 or custom\n","im = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\n","results = model(im)  # inference\n","results.print()  # or .show(), .save(), .crop(), .pandas(), etc."],"execution_count":null,"outputs":[]}]}